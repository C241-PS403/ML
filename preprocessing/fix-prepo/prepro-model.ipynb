{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa\n",
    "import shutil\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'C:\\\\Bangkit\\\\ML\\\\code\\\\preprocessing\\\\dataset'\n",
    "output_directory = 'C:\\\\Bangkit\\\\ML\\\\code\\\\preprocessing\\\\fix-prepo\\\\output'\n",
    "split_directory = 'C:\\\\Bangkit\\\\ML\\\\code\\\\preprocessing\\\\fix-prepo\\\\split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['check.ipynb',\n",
       " 'geblek-renteng',\n",
       " 'gentongan',\n",
       " 'liong',\n",
       " 'mega-mendung',\n",
       " 'parang',\n",
       " 'sekar-jagad',\n",
       " 'sidomukti',\n",
       " 'tambal',\n",
       " 'truntum',\n",
       " 'tujuh-rupa']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(input_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize function\n",
    "def resize_image(image, target_size=(224, 224)):\n",
    "    resized_image = cv2.resize(image, target_size)\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize function\n",
    "def min_max_normalize_image(image):\n",
    "    # Convert image data type to float32\n",
    "    image = image.astype('float32')\n",
    "\n",
    "    # Normalize with Min-Max Scaling\n",
    "    min_val = np.min(image)\n",
    "    max_val = np.max(image)\n",
    "    if min_val == max_val:\n",
    "        return np.zeros_like(image)\n",
    "    else:\n",
    "        normalized_image = (image - min_val) / (max_val - min_val)\n",
    "        return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrast enhancement function\n",
    "def apply_clahe(image, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l_channel, a_channel, b_channel = cv2.split(lab_image)\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    enhanced_l_channel = clahe.apply(l_channel)\n",
    "    clahe_image = cv2.merge((enhanced_l_channel, a_channel, b_channel))\n",
    "    enhanced_image = cv2.cvtColor(clahe_image, cv2.COLOR_LAB2BGR)\n",
    "    return enhanced_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation Function\n",
    "def augment_images(input_folder, output_folder, num_images=10):\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.Affine(rotate=(-20, 20)),\n",
    "        iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "        iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "        iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "        iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5),\n",
    "    ])\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for folder_name in os.listdir(input_folder):\n",
    "        input_folder_path = os.path.join(input_folder, folder_name)\n",
    "        if not os.path.isdir(input_folder_path):\n",
    "            continue\n",
    "        output_folder_path = os.path.join(output_folder, folder_name)\n",
    "        os.makedirs(output_folder_path, exist_ok=True)\n",
    "        image_files = [f for f in os.listdir(input_folder_path) if os.path.isfile(os.path.join(input_folder_path, f))]\n",
    "        selected_files = random.sample(image_files, min(num_images, len(image_files)))\n",
    "        for filename in selected_files:\n",
    "            input_image_path = os.path.join(input_folder_path, filename)\n",
    "            try:\n",
    "                image = cv2.imread(input_image_path)\n",
    "                images_aug = [seq(image=image) for _ in range(num_images)]\n",
    "                for idx, image_aug in enumerate(images_aug):\n",
    "                    output_image_path = os.path.join(output_folder_path, f\"{filename.split('.')[0]}_{idx}.jpg\")\n",
    "                    cv2.imwrite(output_image_path, image_aug)\n",
    "            except Exception as e:\n",
    "                print(f\"Error augmenting image {filename}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom preprocessing function\n",
    "def custom_preprocess(image, seq):\n",
    "    image = resize_image(image)\n",
    "    image = min_max_normalize_image(image)\n",
    "    \n",
    "    # Convert image back to uint8 for CLAHE\n",
    "    image = (image * 255).astype('uint8')\n",
    "    image = apply_clahe(image)\n",
    "    image = seq.augment_image(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process and save images\n",
    "def process_and_save_images(input_dir, output_dir, seq):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                input_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(root, input_dir)\n",
    "                output_folder = os.path.join(output_dir, relative_path)\n",
    "\n",
    "                if not os.path.exists(output_folder):\n",
    "                    os.makedirs(output_folder)\n",
    "\n",
    "                image = cv2.imread(input_path)\n",
    "                if image is not None:\n",
    "                    for i in range(3):\n",
    "                        processed_image = custom_preprocess(image, seq)\n",
    "                        output_filename = f\"{os.path.splitext(file)[0]}_aug{i + 1}.jpg\"\n",
    "                        output_path = os.path.join(output_folder, output_filename)\n",
    "                        cv2.imwrite(output_path, (processed_image * 255).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\imgaug\\imgaug.py:184: DeprecationWarning: Function `ContrastNormalization()` is deprecated. Use `imgaug.contrast.LinearContrast` instead.\n",
      "  warn_deprecated(msg, stacklevel=3)\n"
     ]
    }
   ],
   "source": [
    "# Define augmentation sequence\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Affine(rotate=(-20, 20)),\n",
    "    iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "    iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "    iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and save images\n",
    "process_and_save_images(input_directory, output_directory, seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat folder train, test, dan val di dalam folder split\n",
    "train_dir = os.path.join(split_directory, 'train')\n",
    "test_dir = os.path.join(split_directory, 'test')\n",
    "val_dir = os.path.join(split_directory, 'val')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(input_dir, output_dir, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):\n",
    "    # Make sure output directories exist\n",
    "    train_dir = os.path.join(output_dir, 'train')\n",
    "    val_dir = os.path.join(output_dir, 'val')\n",
    "    test_dir = os.path.join(output_dir, 'test')\n",
    "\n",
    "    for dir_path in [train_dir, val_dir, test_dir]:\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "    # Iterate through each class folder\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        if files:  # If there are files in the directory\n",
    "            # Determine class name based on the relative path\n",
    "            relative_path = os.path.relpath(root, input_dir)\n",
    "            class_name = os.path.basename(relative_path)\n",
    "\n",
    "            # Create corresponding class folders in train, val, test directories\n",
    "            class_train_dir = os.path.join(train_dir, class_name)\n",
    "            class_val_dir = os.path.join(val_dir, class_name)\n",
    "            class_test_dir = os.path.join(test_dir, class_name)\n",
    "\n",
    "            for dir_path in [class_train_dir, class_val_dir, class_test_dir]:\n",
    "                if not os.path.exists(dir_path):\n",
    "                    os.makedirs(dir_path)\n",
    "\n",
    "            # Shuffle files in the current class folder\n",
    "            random.shuffle(files)\n",
    "\n",
    "            # Calculate number of files for train, val, test\n",
    "            num_files = len(files)\n",
    "            num_train = int(train_ratio * num_files)\n",
    "            num_val = int(val_ratio * num_files)\n",
    "            num_test = num_files - num_train - num_val\n",
    "\n",
    "            # Assign files to train, val, test datasets\n",
    "            train_files = files[:num_train]\n",
    "            val_files = files[num_train:num_train + num_val]\n",
    "            test_files = files[num_train + num_val:]\n",
    "\n",
    "            # Move files to corresponding directories\n",
    "            for file in train_files:\n",
    "                src_path = os.path.join(root, file)\n",
    "                dst_path = os.path.join(class_train_dir, file)\n",
    "                shutil.copy(src_path, dst_path)\n",
    "\n",
    "            for file in val_files:\n",
    "                src_path = os.path.join(root, file)\n",
    "                dst_path = os.path.join(class_val_dir, file)\n",
    "                shutil.copy(src_path, dst_path)\n",
    "\n",
    "            for file in test_files:\n",
    "                src_path = os.path.join(root, file)\n",
    "                dst_path = os.path.join(class_test_dir, file)\n",
    "                shutil.copy(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, val, test\n",
    "split_dataset(output_directory, split_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter\n",
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1047 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 149 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 301 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_37512\\2261410678.py:1: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(weights='imagenet', include_top=False)\n"
     ]
    }
   ],
   "source": [
    "base_model = MobileNetV2(weights='imagenet', include_top=False)\n",
    "\n",
    "# Build the model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 533ms/step - accuracy: 0.4035 - loss: 2.1280 - val_accuracy: 0.5570 - val_loss: 1.5205 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 403ms/step - accuracy: 0.7217 - loss: 0.8904 - val_accuracy: 0.6846 - val_loss: 1.0128 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 394ms/step - accuracy: 0.7444 - loss: 0.7778 - val_accuracy: 0.6913 - val_loss: 0.8777 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 770ms/step - accuracy: 0.7815 - loss: 0.6380 - val_accuracy: 0.7450 - val_loss: 0.6720 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 953ms/step - accuracy: 0.8094 - loss: 0.5822 - val_accuracy: 0.7450 - val_loss: 0.6591 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 678ms/step - accuracy: 0.8322 - loss: 0.5243 - val_accuracy: 0.7919 - val_loss: 0.6189 - learning_rate: 0.0010\n",
      "Epoch 7/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 390ms/step - accuracy: 0.8362 - loss: 0.5013 - val_accuracy: 0.8322 - val_loss: 0.5567 - learning_rate: 0.0010\n",
      "Epoch 8/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 391ms/step - accuracy: 0.8522 - loss: 0.4029 - val_accuracy: 0.7919 - val_loss: 0.6077 - learning_rate: 0.0010\n",
      "Epoch 9/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 388ms/step - accuracy: 0.8671 - loss: 0.4018 - val_accuracy: 0.8054 - val_loss: 0.5317 - learning_rate: 0.0010\n",
      "Epoch 10/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 390ms/step - accuracy: 0.8508 - loss: 0.4725 - val_accuracy: 0.8121 - val_loss: 0.5633 - learning_rate: 0.0010\n",
      "Epoch 11/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 386ms/step - accuracy: 0.8699 - loss: 0.3581 - val_accuracy: 0.7852 - val_loss: 0.6671 - learning_rate: 0.0010\n",
      "Epoch 12/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 390ms/step - accuracy: 0.8478 - loss: 0.4282 - val_accuracy: 0.7584 - val_loss: 0.7691 - learning_rate: 0.0010\n",
      "Epoch 13/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 387ms/step - accuracy: 0.8734 - loss: 0.3951 - val_accuracy: 0.7651 - val_loss: 0.7474 - learning_rate: 0.0010\n",
      "Epoch 14/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 394ms/step - accuracy: 0.8849 - loss: 0.3283 - val_accuracy: 0.7785 - val_loss: 0.7249 - learning_rate: 0.0010\n",
      "Epoch 15/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 391ms/step - accuracy: 0.8733 - loss: 0.3699 - val_accuracy: 0.7852 - val_loss: 0.7251 - learning_rate: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune some layers of the base model\n",
    "for layer in base_model.layers[-30:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 491ms/step - accuracy: 0.6577 - loss: 1.1083 - val_accuracy: 0.7919 - val_loss: 0.5961 - learning_rate: 1.0000e-05\n",
      "Epoch 2/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 460ms/step - accuracy: 0.6917 - loss: 0.9640 - val_accuracy: 0.7651 - val_loss: 0.6713 - learning_rate: 1.0000e-05\n",
      "Epoch 3/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 466ms/step - accuracy: 0.7379 - loss: 0.8435 - val_accuracy: 0.7517 - val_loss: 0.7253 - learning_rate: 1.0000e-05\n",
      "Epoch 4/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 467ms/step - accuracy: 0.7423 - loss: 0.7944 - val_accuracy: 0.7383 - val_loss: 0.7781 - learning_rate: 1.0000e-05\n",
      "Epoch 5/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 457ms/step - accuracy: 0.7492 - loss: 0.7216 - val_accuracy: 0.7248 - val_loss: 0.8092 - learning_rate: 1.0000e-05\n",
      "Epoch 6/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 475ms/step - accuracy: 0.7585 - loss: 0.7305 - val_accuracy: 0.7181 - val_loss: 0.8494 - learning_rate: 1.0000e-05\n",
      "Epoch 7/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 461ms/step - accuracy: 0.7666 - loss: 0.6866 - val_accuracy: 0.7248 - val_loss: 0.8504 - learning_rate: 2.0000e-06\n",
      "Epoch 8/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 462ms/step - accuracy: 0.8026 - loss: 0.5705 - val_accuracy: 0.7315 - val_loss: 0.8451 - learning_rate: 2.0000e-06\n",
      "Epoch 9/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 463ms/step - accuracy: 0.7917 - loss: 0.7207 - val_accuracy: 0.7450 - val_loss: 0.8438 - learning_rate: 2.0000e-06\n",
      "Epoch 10/15\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 479ms/step - accuracy: 0.7583 - loss: 0.7631 - val_accuracy: 0.7584 - val_loss: 0.8348 - learning_rate: 2.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# Recompile the model with a lower learning rate for fine-tuning\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate * 0.1), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "history_fine_tune = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 4s - 415ms/step - accuracy: 0.7674 - loss: 0.8327\n",
      "Test accuracy: 0.7674418687820435\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(test_generator, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
